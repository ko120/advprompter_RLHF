defaults:
  - prompter: llama2  # one of: llama2, tiny_llama, dummy_llama
  - target_llm: vicuna_chat  # one of: llama2_chat, vicuna_chat, mistral_chat, pythia_chat, falcon_chat, gemma_chat, tiny_llama_chat, dummy_llama_chat
  - data: multi-adaptive # one of: multi-adaptive, single-fixed, dummy
  - _self_
  - override hydra/launcher: submitit_slurm

prompter:
  llm_params:
    device: 'cuda:0'
    for_ppo: false
    freeze: false
    reset_head: false
    dtype: float32
    gradient_checkpointing: false
    save_base_path: '<checkpoints_path>/${target_llm.llm_params.model_name}'
    quantization_type: "no quantization" # str: 4bit or 8bit, other types will be ignored
  allow_non_ascii: false
  gen_params:
    do_sample: true  # default in llama2 is true
    temperature: 1.0  # default in llama2 is 0.6
    top_p: 0.9  # default in llama2 is 0.9
    # num_beams: 4
    # num_beams_groups: 4
    # top_k: 32
    # repetition_penalty: 1.2

target_llm:
  llm_params:
    device: 'cuda:1'
    for_ppo: false
    freeze: true
    reset_head: false
    dtype: float16
    gradient_checkpointing: false
    save_base_path: null
    lora_params: null
    quantization_type: "no quantization" # str: 4bit or 8bit, other types will be ignored
  allow_non_ascii: true
  gen_params:
    max_new_tokens: 150
    do_sample: false  # default true
    # temperature: 0.0  # default 0.6
    # num_beams: 1
    # top_k: 0
    # top_p: 0.0  # default 0.9
    # repetition_penalty: 1.0

data:
  experiment_name: harmful_behaviors  # one of: harmful_strings, harmful_behaviors
  data_dir: "./data"
  test_prefixes_pth: "${data.data_dir}/test_prefixes.csv"
  affirmative_prefixes_pth: "${data.data_dir}/affirmative_prefixes.csv"
  train:
    dataloader:
      batch_size: 8
      num_workers: 0
  eval:
    dataloader:
      batch_size: 8
      num_workers: 0
  test:
    dataloader:
      batch_size: 8
      num_workers: 0

eval:
  augment_target: false
  do_initial_eval: true
  do_final_eval: true
  save_trained_model: true
  also_evaluate_on_train: true
  eval_every: 1
  max_eval_trials: 3 # number of sampling performed per iter
  load_prompter: true
  eval_base_name: "prompterLLM_response_${target_llm.llm_params.model_name}_"
  prompter:
    max_new_tokens_list:
      - 30
      - 50

verbose: false
seed: 2023
wandb_params:
  log_sequences_every: 
    train: 1000
    eval: 1000
  log_table_at_epoch_end_only: true  # should be true for multi-epoch runs
  enable_wandb: true
  watch_params: null
    # log: "gradients"
    # log_freq: 39

hydra:
  run:
    dir: ./exp/local/${now:%Y.%m.%d}/${now:%H%M}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${hydra.runtime.choices.data}_${now:%H%M}
    subdir: ${hydra.job.num}
  # job:
    # chdir: True
  launcher:
    max_num_timeout: 100000
    timeout_min: 4319
    # partition: scavenge  # one of devlab, learnlab, learnfair, scavenge
    partition: <slurm_partition> #
    mem_gb: 64
    gpus_per_node: 1
    # constraint: volta32gb
