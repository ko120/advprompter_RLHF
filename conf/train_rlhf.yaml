defaults:
  - value: harm_bench
  - reward: harm_bench
  - eval
  - _self_

mode: train


pretrain:
  enable: false
  epochs: 20
  batch_size: 8
  dataset_key: pretrain
  dataset_pth: ... # path to csv, should be in the same format as suffix_opt_dataset (i.e. contain column suffix)
  do_eval_after: true

train:
  epochs: 10
  batch_size: 8 # need to have per_device_train_batch_size * gradient_accumulation_steps * num_mini_batches
  enable_accelerator: True
  dataset_key: foo
  dataset_pth: "${data.data_dir}/harmful_behaviors/dataset/train.csv"
  suffix_opt_dataset_dir: "${output_dir}/suffix_opt_dataset"
  do_initial_eval: false
  eval_every: 2
  model_save_dir: "${output_dir}/checkpoints"
  augment_target: true
  replay_buffer:
    num_updates: 8
    size: 256
    priority_alpha:  1.5
    # priority = priority_factor.loss_delta * relu(loss_delta) + priority_factor.jailbreaking * jailbreaking
    priority_factor:  # note: zero priority are not added to buffer
      loss_delta: 1.0
      jailbreaking: 1.0
  prompter_optim_params:
    lr: 5e-4
  q_params:
    max_new_tokens: 30
    num_beams: 4
    repetition_penalty: 1.2
    top_k: 24 # try to reduce this or increase num_chunks if doesn't fit to memory
    num_chunks: 1  # process top_k iteratively in chunks, helps reduce memory, should divide top_k
    lambda_val: 100  # w2 in AutoDAN paper, controls perplexity vs loss tradeoff (50-100 is good)
    candidates:
      do_sample: true
      temperature: 0.6
      always_include_best: true
    beams:
      do_sample: true
      temperature: 0.6
      always_include_best: true
  # trl=0.12.1
  ppo_params:
    use_jb_as_score: true
    max_loss_const: 1.0
    config:
      output_dir: "${output_dir}/"
      learning_rate: 3e-5
      gamma: 1
      lam: 0.95
      kl_coef: 0.05 # scales between kl score reward and reward generated score
      vf_coef: 0.01 # scale between policy loss and value loss 
      # gradient_accumulation_steps: 2
      missing_eos_penalty: 1.0
      # for deep speed
      num_ppo_epochs: 1
      num_mini_batches: 1 # ppo iter batch
      per_device_train_batch_size: 4
      gradient_accumulation_steps: 2
      local_rollout_forward_batch_size: 2
    gen_params:
      do_sample: true
      temperature: 0.7
      top_p: 0.92
      max_new_tokens: 30

  # trl=0.11.4
  # ppo_params:
  #   use_jb_as_score: true
  #   max_loss_const: 1.0
  #   config:
  #     learning_rate: 5e-5
  #     gamma: 0.95
  #     lam: 0.95
  #     mini_batch_size: 2
  #     gradient_accumulation_steps: 2
  #   gen_params:
  #     do_sample: true
  #     temperature: 0.6
  #     top_p: 0.9
  #     max_new_tokens: 30
